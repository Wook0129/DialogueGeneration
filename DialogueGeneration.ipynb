{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from corpus.opensubsdata import OpensubsData\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenSubtitles conversations in data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data\\OpenSubtitles\\en\\Action\\2003\\602_152466_207871_batoru_rowaiaru_ii_rekuiemu.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data\\OpenSubtitles\\en\\Action\\2004\\59_84873_113518_appurushdo.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data\\OpenSubtitles\\en\\Comedy\\2003\\529_124078_171007_how_to_lose_a_guy_in_10_days.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data\\OpenSubtitles\\en\\Comedy\\2004\\2480_226704_299940_little_black_book.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data\\OpenSubtitles\\en\\Drama\\2000\\179_88528_119102_batoru_rowaiaru.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data\\OpenSubtitles\\en\\Drama\\2002\\3265_149497_204017_unfaithful.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data\\OpenSubtitles\\en\\Drama\\2003\\1723_68784_89159_big_fish.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data\\OpenSubtitles\\en\\Drama\\2004\\146_206647_272090_eternal_sunshine_of_the_spotless_mind.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data\\OpenSubtitles\\en\\Family\\2001\\3935_19508_22105_cats__dogs.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data\\OpenSubtitles\\en\\Horror\\1922\\1166_134135_184270_nosferatu_eine_symphonie_des_grauens.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2029/2029 [02:25<00:00, 13.98it/s]\n"
     ]
    }
   ],
   "source": [
    "data = OpensubsData('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1468840"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.getConversations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "#     text = re.sub('[0-9]', '[NUMBER]')\n",
    "    text = re.sub('[^a-zA-Z ]', '', text)\n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1468840/1468840 [00:14<00:00, 102060.75it/s]\n"
     ]
    }
   ],
   "source": [
    "conversations = []\n",
    "for conversation in tqdm(data.getConversations()):\n",
    "    q = conversation['lines'][0]['text']\n",
    "    a = conversation['lines'][1]['text']\n",
    "    conversations.append([normalize(q), normalize(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['why', 'it', 'won', 't', 'work'], ['this', 'is', 'the', 'northern', 'hemisphere', 'correct']]\n"
     ]
    }
   ],
   "source": [
    "print(conversations[5004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_dict(conversations):\n",
    "    vocab = Counter()\n",
    "    for conversation in conversations:\n",
    "        for sentence in conversation:\n",
    "            vocab.update(sentence)\n",
    "\n",
    "    unk_word_cutoff_freq = 10\n",
    "    sorted_words = ['_'] + sorted([word for word in vocab.keys() \n",
    "                                   if vocab[word] > unk_word_cutoff_freq])\n",
    "    index_to_word = dict(enumerate(sorted_words))\n",
    "    word_to_index = dict((v, k) for k,v in index_to_word.items())\n",
    "    \n",
    "    vocab_size = len(index_to_word)\n",
    "    index_to_word[vocab_size] = 'UNK'\n",
    "    word_to_index['UNK'] = vocab_size\n",
    "    \n",
    "    return index_to_word, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encode_conversations(conversations, word_to_index):\n",
    "\n",
    "    \"\"\"\n",
    "    Input//  \n",
    "    conversations = List of conversation\n",
    "        conversation = [sentence1, sentence2]\n",
    "        sentence = List of String\n",
    "    word_to_index = Dictionary, key = word(string), value = index(int)\n",
    "    \n",
    "    Output//\n",
    "    encoded_conversations = List of encoded_conversation\n",
    "        enconded_conversation = [encoded_sentence1, encoded_sentence2]\n",
    "        encoded_sentence = List of Int, Length == Maximum Length of Sentence in Training Data\n",
    "    \"\"\"\n",
    "    \n",
    "    encoded_conversations = []\n",
    "    max_sentence_len = max(len(sentence) for conversation in conversations \n",
    "                                           for sentence in conversation)\n",
    "    \n",
    "    def encode_sentence(sentence, word_to_index):\n",
    "        encoded_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in word_to_index.keys():\n",
    "                encoded_sentence.append(word_to_index[word])\n",
    "            else:\n",
    "                encoded_sentence.append(word_to_index['UNK'])\n",
    "        # Sentence Length == max_len\n",
    "        encoded_sentence += [0] * (max_sentence_len - len(encoded_sentence))\n",
    "        return encoded_sentence\n",
    "\n",
    "    for conversation in conversations:\n",
    "        encoded_conversations.append([encode_sentence(sentence, word_to_index) \n",
    "                                          for sentence in conversation])\n",
    "    return encoded_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_to_word, word_to_index = make_dict(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31555"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31555"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = encode_conversations(conversations, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12778, 21009, 4377, 12407, 23961, 1628, 28417, 26235, 28326, 30992, 17682, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17682, 14920, 3831, 12907, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_length = len(data[0][0])\n",
    "batch_size = 32\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encode_input = [tf.placeholder(tf.int32, shape=(None,), name = \"ei_%i\" %i) \n",
    "                    for i in range(sentence_length)]\n",
    "labels = [tf.placeholder(tf.int32, shape=(None,), name = \"l_%i\" %i)\n",
    "                    for i in range(sentence_length)]\n",
    "decode_input = [tf.zeros_like(encode_input[0], dtype=tf.int32, name=\"GO\")] + labels[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cells = [tf.contrib.rnn.GRUCell(embedding_dim) for i in range(1)]\n",
    "# stacked_gru = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "stacked_gru = cells[0] # At now, Use Just One Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq2seq_config = {\n",
    "    'encoder_inputs':encode_input,\n",
    "    'decoder_inputs':decode_input,\n",
    "    'cell':stacked_gru,\n",
    "    'num_encoder_symbols':vocab_size,\n",
    "    'num_decoder_symbols':vocab_size,\n",
    "    'embedding_size':embedding_dim\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoders\") as scope:\n",
    "    decode_outputs, _ = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(**seq2seq_config, \n",
    "                                                                        feed_previous=False)\n",
    "    scope.reuse_variables()\n",
    "    decode_outputs_test, _ = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(**seq2seq_config,\n",
    "                                                                             feed_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_weights = [tf.ones_like(l, dtype=tf.float32) for l in labels]\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(decode_outputs, labels, loss_weights, vocab_size)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    \n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.iter = self.make_random_iter()\n",
    "\n",
    "    def make_random_iter(self):\n",
    "        splits = np.arange(self.batch_size, len(self.data), self.batch_size)\n",
    "        it = np.split(np.random.permutation(range(len(self.data))), splits)[:-1]\n",
    "        return iter(it)\n",
    "        \n",
    "    def next_batch(self):\n",
    "        try:\n",
    "            idxs = next(self.iter)\n",
    "        except StopIteration:\n",
    "            self.iter = self.make_random_iter()\n",
    "            idxs = next(self.iter)\n",
    "        \n",
    "        X = [self.data[i][0] for i in idxs]\n",
    "        Y = [self.data[i][1] for i in idxs]\n",
    "        X = np.array(X).T\n",
    "        Y = np.array(Y).T\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_feed(X, Y):\n",
    "    feed_dict = {encode_input[t]: X[t] for t in range(sentence_length)}\n",
    "    feed_dict.update({labels[t]: Y[t] for t in range(sentence_length)})\n",
    "    return feed_dict\n",
    "\n",
    "def train_batch(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    _, out = sess.run([train_op, loss], feed_dict)\n",
    "    return out\n",
    "\n",
    "def get_eval_batch_data(data_iter):\n",
    "    X, Y = data_iter.next_batch()\n",
    "    train_iter = BatchGenerator(train_data, 32)\n",
    "    val_iter = BatchGenerator(val_data, 10)\n",
    "    test_iter = BatchGenerator(test_data, 10)\n",
    "    feed_dict = get_feed(X, Y)\n",
    "    all_output = sess.run([loss] + decode_outputs_test, feed_dict)\n",
    "    eval_loss = all_output[0]\n",
    "    decode_output = np.array(all_output[1:]).transpose([1,0,2])\n",
    "    return eval_loss, decode_output, X, Y\n",
    "\n",
    "def eval_batch(data_iter, num_batches):\n",
    "    losses = []\n",
    "    predict_loss = []\n",
    "    for i in range(num_batches):\n",
    "        eval_loss, output, X, Y = get_eval_batch_data(data_iter)\n",
    "        losses.append(eval_loss)\n",
    "\n",
    "        for index in range(len(output)):\n",
    "            real = Y.T[index]\n",
    "            predict = np.argmax(output, axis = 2)[index]\n",
    "            predict_loss.append(all(real==predict))\n",
    "    return np.mean(losses), np.mean(predict_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468840\n"
     ]
    }
   ],
   "source": [
    "num_instance = len(data)\n",
    "print(num_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = data[:int(num_instance * 0.6)]\n",
    "val_data = data[int(num_instance * 0.6) : int(num_instance * 0.9)]\n",
    "test_data = data[int(num_instance * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_iter = BatchGenerator(train_data, 32)\n",
    "val_iter = BatchGenerator(train_data, 10)\n",
    "test_iter = BatchGenerator(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0.000000\n",
      "val loss   : 10.355874\n",
      "train loss : 10.355784\n",
      "iteration : 5.000000\n",
      "val loss   : 10.347193\n",
      "train loss : 10.346895\n",
      "iteration : 10.000000\n",
      "val loss   : 10.336492\n",
      "train loss : 10.336823\n",
      "iteration : 15.000000\n",
      "val loss   : 10.322632\n",
      "train loss : 10.323482\n",
      "iteration : 5705.000000\n",
      "val loss   : 1.163440\n",
      "train loss : 1.108397\n",
      "iteration : 5710.000000\n",
      "val loss   : 1.041192\n",
      "train loss : 1.090889\n",
      "iteration : 5715.000000\n",
      "val loss   : 1.100498\n",
      "train loss : 1.054541\n",
      "iteration : 5720.000000\n",
      "val loss   : 1.105267\n",
      "train loss : 1.099669\n",
      "iteration : 5725.000000\n",
      "val loss   : 1.104627\n",
      "train loss : 1.074680\n",
      "iteration : 5730.000000\n",
      "val loss   : 1.051386\n",
      "train loss : 1.054379\n",
      "iteration : 5735.000000\n",
      "val loss   : 1.121879\n",
      "train loss : 1.043492\n",
      "iteration : 5740.000000\n",
      "val loss   : 1.165042\n",
      "train loss : 1.088795\n",
      "iteration : 5745.000000\n",
      "val loss   : 1.115464\n",
      "train loss : 1.079772\n",
      "iteration : 5750.000000\n",
      "val loss   : 1.103851\n",
      "train loss : 1.072879\n",
      "iteration : 5755.000000\n",
      "val loss   : 1.121076\n",
      "train loss : 1.100457\n",
      "iteration : 5760.000000\n",
      "val loss   : 1.045701\n",
      "train loss : 1.060977\n",
      "iteration : 5765.000000\n",
      "val loss   : 1.173550\n",
      "train loss : 1.067344\n",
      "iteration : 5770.000000\n",
      "val loss   : 1.099069\n",
      "train loss : 1.088160\n",
      "iteration : 5775.000000\n",
      "val loss   : 1.023240\n",
      "train loss : 1.051919\n",
      "iteration : 5780.000000\n",
      "val loss   : 1.086573\n",
      "train loss : 1.083355\n",
      "iteration : 5785.000000\n",
      "val loss   : 1.057151\n",
      "train loss : 1.017390\n",
      "iteration : 5790.000000\n",
      "val loss   : 1.165780\n",
      "train loss : 1.097350\n",
      "iteration : 5795.000000\n",
      "val loss   : 1.145116\n",
      "train loss : 1.067418\n",
      "iteration : 5800.000000\n",
      "val loss   : 1.183414\n",
      "train loss : 1.068739\n",
      "iteration : 5805.000000\n",
      "val loss   : 1.053082\n",
      "train loss : 1.084876\n",
      "iteration : 5810.000000\n",
      "val loss   : 1.026076\n",
      "train loss : 1.126467\n",
      "iteration : 5815.000000\n",
      "val loss   : 0.981614\n",
      "train loss : 1.093403\n",
      "iteration : 5820.000000\n",
      "val loss   : 1.065875\n",
      "train loss : 1.095443\n",
      "iteration : 5825.000000\n",
      "val loss   : 1.060835\n",
      "train loss : 1.121007\n",
      "iteration : 5830.000000\n",
      "val loss   : 1.157795\n",
      "train loss : 1.109710\n",
      "iteration : 5835.000000\n",
      "val loss   : 1.060167\n",
      "train loss : 1.118169\n",
      "iteration : 5840.000000\n",
      "val loss   : 1.051054\n",
      "train loss : 1.021512\n",
      "iteration : 5845.000000\n",
      "val loss   : 1.090679\n",
      "train loss : 1.085426\n",
      "iteration : 5850.000000\n",
      "val loss   : 1.018702\n",
      "train loss : 1.102397\n",
      "iteration : 5855.000000\n",
      "val loss   : 1.136456\n",
      "train loss : 1.132262\n",
      "iteration : 5860.000000\n",
      "val loss   : 0.876278\n",
      "train loss : 1.097603\n",
      "iteration : 5865.000000\n",
      "val loss   : 1.065844\n",
      "train loss : 1.091076\n",
      "iteration : 5870.000000\n",
      "val loss   : 1.083676\n",
      "train loss : 1.058912\n",
      "iteration : 5875.000000\n",
      "val loss   : 1.107875\n",
      "train loss : 1.067032\n",
      "iteration : 5880.000000\n",
      "val loss   : 1.100435\n",
      "train loss : 1.035084\n",
      "iteration : 5885.000000\n",
      "val loss   : 1.131263\n",
      "train loss : 1.095305\n",
      "iteration : 5890.000000\n",
      "val loss   : 1.069837\n",
      "train loss : 1.063144\n",
      "iteration : 5895.000000\n",
      "val loss   : 1.050263\n",
      "train loss : 1.077779\n",
      "iteration : 5900.000000\n",
      "val loss   : 1.093273\n",
      "train loss : 1.139072\n",
      "iteration : 5905.000000\n",
      "val loss   : 1.076977\n",
      "train loss : 1.083341\n",
      "iteration : 5910.000000\n",
      "val loss   : 0.974018\n",
      "train loss : 1.061656\n",
      "iteration : 5915.000000\n",
      "val loss   : 1.055756\n",
      "train loss : 1.074355\n",
      "iteration : 5920.000000\n",
      "val loss   : 1.120363\n",
      "train loss : 1.046257\n",
      "iteration : 5925.000000\n",
      "val loss   : 1.033829\n",
      "train loss : 1.135689\n",
      "iteration : 5930.000000\n",
      "val loss   : 1.159215\n",
      "train loss : 1.061232\n",
      "interrupted by user\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    try:\n",
    "        train_batch(train_iter)\n",
    "        if i % 5 == 0:\n",
    "            val_loss, val_predict = eval_batch(val_iter, 10)\n",
    "            train_loss, train_predict = eval_batch(train_iter, 10)\n",
    "            \n",
    "            print ('iteration : %f' %(i))\n",
    "            print ('val loss   : %f' %(val_loss))\n",
    "            print ('train loss : %f' %(train_loss))\n",
    "#             print\n",
    "#             sys.stdout.flush()\n",
    "#             if i % 2000 == 0:\n",
    "#                 show_text('train')\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print (\"interrupted by user\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_text(data_type):\n",
    "    if data_type == 'train':\n",
    "        eval_loss, output, X, Y = get_eval_batch_data(train_iter)\n",
    "    elif data_type == 'test':\n",
    "        eval_loss, output, X, Y = get_eval_batch_data(test_iter)\n",
    "\n",
    "    for index in range(len(output)):\n",
    "        input_text = \" \".join([index_to_word[p] for p in X.T[index]])\n",
    "        label_text = \" \".join([index_to_word[l] for l in Y.T[index]])\n",
    "        predicted_text = \" \".join([index_to_word[l] for l in np.argmax(output, axis = 2)[index]])\n",
    "\n",
    "        print (input_text.split(\"_\")[0].ljust(40))\n",
    "        print (\"\".join(label_text).split(\"_\")[0].ljust(17))\n",
    "        print (\"\".join(predicted_text).split(\"_\")[0].ljust(17), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        \n",
      "here we go here we go go UNK go megget \n",
      "                  \n",
      "\n",
      "go ahead surprise me                    \n",
      "sorry chief that only gets you halfway there \n",
      "i t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "\n",
      "lf your child s school has old ass books \n",
      "and brand new metal detectors let me hear you say that ain t right \n",
      "i m t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "\n",
      "really i m serious                      \n",
      "anyway why did she faint \n",
      "i t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "\n",
      "i believe a light that shines on you    \n",
      "will shine on you forever \n",
      "i m t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "\n",
      "but don t put your hand down there      \n",
      "i didn t mean    \n",
      "i m t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "\n",
      "wives and family and stuff yeah i mean not children \n",
      "but their wives and girlfriends et cetera \n",
      "i m t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "\n",
      "well that ought to do it                \n",
      "look at that and he fixed my neck \n",
      "i m t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "\n",
      "is that ernie                           \n",
      "it s ernie       \n",
      "i t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "\n",
      "of the misery right                     \n",
      "oh what an exit  \n",
      "i t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_text('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
